%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Rodrigo Nemmen da Silva at 2023-01-27 12:11:09 -0800 


%% Saved with string encoding Unicode (UTF-8) 


@comment{jabref-meta: databaseType:bibtex;}



@article{Almeida2022,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {https://ui.adsabs.harvard.edu/abs/2022MNRAS.509.5657A},
	author = {{Almeida}, Ivan and {Duarte}, Roberta and {Nemmen}, Rodrigo},
	date-added = {2023-01-04 15:37:40 -0800},
	date-modified = {2023-01-04 15:37:41 -0800},
	doi = {10.1093/mnras/stab3353},
	journal = {\mnras},
	keywords = {accretion discs, black hole physics, methods: statistical, galaxies: active},
	month = feb,
	number = {4},
	pages = {5657-5668},
	title = {{Deep learning Bayesian inference for low-luminosity active galactic nuclei spectra}},
	volume = {509},
	year = 2022,
	bdsk-url-1 = {https://doi.org/10.1093/mnras/stab3353}}

@article{Duarte2022,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {https://ui.adsabs.harvard.edu/abs/2022MNRAS.512.5848D},
	archiveprefix = {arXiv},
	author = {{Duarte}, Roberta and {Nemmen}, Rodrigo and {Navarro}, Jo{\~a}o Paulo},
	date-added = {2023-01-04 15:37:19 -0800},
	date-modified = {2023-01-04 15:37:19 -0800},
	doi = {10.1093/mnras/stac665},
	eprint = {2102.06242},
	journal = {\mnras},
	keywords = {accretion, accretion discs, black hole physics, hydrodynamics, MHD, methods: numerical, methods: statistical, Astrophysics - High Energy Astrophysical Phenomena},
	month = jun,
	number = {4},
	pages = {5848-5861},
	primaryclass = {astro-ph.HE},
	title = {{Black hole weather forecasting with deep learning: a pilot study}},
	volume = {512},
	year = 2022,
	bdsk-url-1 = {https://doi.org/10.1093/mnras/stac665}}

@inproceedings{Stachenfeld2022,
	author = {Kim Stachenfeld and Drummond Buschman Fielding and Dmitrii Kochkov and Miles Cranmer and Tobias Pfaff and Jonathan Godwin and Can Cui and Shirley Ho and Peter Battaglia and Alvaro Sanchez-Gonzalez},
	booktitle = {International Conference on Learning Representations},
	date-added = {2022-07-07 17:08:32 -0300},
	date-modified = {2022-07-07 17:08:33 -0300},
	title = {Learned Simulators for Turbulence},
	url = {https://openreview.net/forum?id=msRBojTz-Nh},
	year = {2022},
	bdsk-url-1 = {https://openreview.net/forum?id=msRBojTz-Nh}}

@misc{Li2020,
	author = {Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-07-07 17:07:41 -0300},
	date-modified = {2022-07-07 17:07:42 -0300},
	doi = {10.48550/ARXIV.2010.08895},
	keywords = {Machine Learning (cs.LG), Numerical Analysis (math.NA), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
	publisher = {arXiv},
	title = {Fourier Neural Operator for Parametric Partial Differential Equations},
	url = {https://arxiv.org/abs/2010.08895},
	year = {2020},
	bdsk-url-1 = {https://arxiv.org/abs/2010.08895},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2010.08895}}

@inproceedings{Zhu2021,
	author = {Zhu, Chengke and Ye, Hongxia and Zhan, Bin},
	booktitle = {2021 Photonics & Electromagnetics Research Symposium (PIERS)},
	date-added = {2022-07-07 17:07:25 -0300},
	date-modified = {2022-07-07 17:07:27 -0300},
	doi = {10.1109/PIERS53385.2021.9695119},
	pages = {1635-1643},
	title = {Fast Solver of 2D Maxwell's Equations Based on Fourier Neural Operator},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/PIERS53385.2021.9695119}}

@misc{Pathak2022,
	author = {Pathak, Jaideep and Subramanian, Shashank and Harrington, Peter and Raja, Sanjeev and Chattopadhyay, Ashesh and Mardani, Morteza and Kurth, Thorsten and Hall, David and Li, Zongyi and Azizzadenesheli, Kamyar and Hassanzadeh, Pedram and Kashinath, Karthik and Anandkumar, Animashree},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-07-07 17:06:35 -0300},
	date-modified = {2022-07-07 17:06:36 -0300},
	doi = {10.48550/ARXIV.2202.11214},
	keywords = {Atmospheric and Oceanic Physics (physics.ao-ph), Machine Learning (cs.LG), FOS: Physical sciences, FOS: Physical sciences, FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {FourCastNet: A Global Data-driven High-resolution Weather Model using Adaptive Fourier Neural Operators},
	url = {https://arxiv.org/abs/2202.11214},
	year = {2022},
	bdsk-url-1 = {https://arxiv.org/abs/2202.11214},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2202.11214}}

@article{Cranmer2021,
	abstract = {Despite over 300 y of effort, no solutions exist for predicting when a general planetary configuration will become unstable. We introduce a deep learning architecture to push forward this problem for compact systems. While current machine learning algorithms in this area rely on scientist-derived instability metrics, our new technique learns its own metrics from scratch, enabled by a internal structure inspired from dynamics theory. Our model can quickly and accurately predict instability timescales in compact multiplanet systems, and does so with an accurate uncertainty estimate for unfamiliar systems. This opens up the development of fast terrestrial planet formation models, and enables the efficient exploration of stable regions in parameter space for multiplanet systems.We introduce a Bayesian neural network model that can accurately predict not only if, but also when a compact planetary system with three or more planets will go unstable. Our model, trained directly from short N-body time series of raw orbital elements, is more than two orders of magnitude more accurate at predicting instability times than analytical estimators, while also reducing the bias of existing machine learning algorithms by nearly a factor of three. Despite being trained on compact resonant and near-resonant three-planet configurations, the model demonstrates robust generalization to both nonresonant and higher multiplicity configurations, in the latter case outperforming models fit to that specific set of integrations. The model computes instability estimates up to 105 times faster than a numerical integrator, and unlike previous efforts provides confidence intervals on its predictions. Our inference model is publicly available in the SPOCK (https://github.com/dtamayo/spock) package, with training code open sourced (https://github.com/MilesCranmer/bnn_chaos_model).Simulation/numerical integration output data have been deposited in GitHub (https://github.com/MilesCranmer/bnn_chaos_model). Our inference model is publicly available in the SPOCK (https://github.com/dtamayo/spock) package. Data are currently publicly available on Zenodo (https://zenodo.org/record/5501473) (72). This work made use of several Python software packages: numpy (73), scipy (74), sklearn (75), jupyter (76), matplotlib (77), pandas (78), torch (71), pytorch-lightning (79), and tensorflow (80).},
	author = {Cranmer, Miles and Tamayo, Daniel and Rein, Hanno and Battaglia, Peter and Hadden, Samuel and Armitage, Philip J. and Ho, Shirley and Spergel, David N.},
	date-added = {2022-02-23 18:10:13 -0600},
	date-modified = {2022-02-23 18:10:14 -0600},
	doi = {10.1073/pnas.2026053118},
	elocation-id = {e2026053118},
	eprint = {https://www.pnas.org/content/118/40/e2026053118.full.pdf},
	issn = {0027-8424},
	journal = {Proceedings of the National Academy of Sciences},
	number = {40},
	publisher = {National Academy of Sciences},
	title = {A Bayesian neural network predicts the dissolution of compact planetary systems},
	url = {https://www.pnas.org/content/118/40/e2026053118},
	volume = {118},
	year = {2021},
	bdsk-url-1 = {https://www.pnas.org/content/118/40/e2026053118},
	bdsk-url-2 = {https://doi.org/10.1073/pnas.2026053118}}

@article{Schmidt2009,
	author = {Michael Schmidt and Hod Lipson},
	date = {2009},
	date-added = {2021-10-20 23:05:39 -0300},
	date-modified = {2021-10-20 23:05:40 -0300},
	journal = {Science},
	number = {5923},
	pages = {81-85},
	title = {Distilling Free-Form Natural Laws from Experimental Data},
	type = {doi:10.1126/science.1165893},
	url = {https://www.science.org/doi/abs/10.1126/science.1165893 %X For centuries, scientists have attempted to identify and document analytical laws that underlie physical phenomena in nature. Despite the prevalence of computing power, the process of finding natural laws and their corresponding equations has resisted automation. A key challenge to finding analytic relations automatically is defining algorithmically what makes a correlation in observed data important and insightful. We propose a principle for the identification of nontriviality. We demonstrated this approach by automatically searching motion-tracking data captured from various physical systems, ranging from simple harmonic oscillators to chaotic double-pendula. Without any prior knowledge about physics, kinematics, or geometry, the algorithm discovered Hamiltonians, Lagrangians, and other laws of geometric and momentum conservation. The discovery rate accelerated as laws found for simpler systems were used to bootstrap explanations for more complex systems, gradually uncovering the “alphabet” used to describe those systems.},
	volume = {324},
	year = {2009}}

@article{Ravuri2021,
	abstract = {Precipitation nowcasting, the high-resolution forecasting of precipitation up to two hours ahead, supports the real-world socioeconomic needs of many sectors reliant on weather-dependent decision-making1,2. State-of-the-art operational nowcasting methods typically advect precipitation fields with radar-based wind estimates, and struggle to capture important non-linear events such as convective initiations3,4. Recently introduced deep learning methods use radar to directly predict future rain rates, free of physical constraints5,6. While they accurately predict low-intensity rainfall, their operational utility is limited because their lack of constraints produces blurry nowcasts at longer lead times, yielding poor performance on rarer medium-to-heavy rain events. Here we present a deep generative model for the probabilistic nowcasting of precipitation from radar that addresses these challenges. Using statistical, economic and cognitive measures, we show that our method provides improved forecast quality, forecast consistency and forecast value. Our model produces realistic and spatiotemporally consistent predictions over regions up to 1,536 km ×1,280 km and with lead times from 5--90 min ahead. Using a systematic evaluation by more than 50 expert meteorologists, we show that our generative model ranked first for its accuracy and usefulness in 89{\%} of cases against two competitive methods. When verified quantitatively, these nowcasts are skillful without resorting to blurring. We show that generative nowcasting can provide probabilistic predictions that improve forecast value and support operational utility, and at resolutions and lead times where alternative methods struggle.},
	author = {Ravuri, Suman and Lenc, Karel and Willson, Matthew and Kangin, Dmitry and Lam, Remi and Mirowski, Piotr and Fitzsimons, Megan and Athanassiadou, Maria and Kashem, Sheleem and Madge, Sam and Prudden, Rachel and Mandhane, Amol and Clark, Aidan and Brock, Andrew and Simonyan, Karen and Hadsell, Raia and Robinson, Niall and Clancy, Ellen and Arribas, Alberto and Mohamed, Shakir},
	da = {2021/09/01},
	date-added = {2021-10-11 16:42:09 -0300},
	date-modified = {2021-10-11 16:42:10 -0300},
	doi = {10.1038/s41586-021-03854-z},
	id = {Ravuri2021},
	isbn = {1476-4687},
	journal = {Nature},
	number = {7878},
	pages = {672--677},
	title = {Skilful precipitation nowcasting using deep generative models of radar},
	ty = {JOUR},
	url = {https://doi.org/10.1038/s41586-021-03854-z},
	volume = {597},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1038/s41586-021-03854-z}}

@article{Kelley1960,
	author = {Kelley, Henry J},
	date-added = {2021-09-16 12:38:08 -0300},
	date-modified = {2021-09-16 12:38:08 -0300},
	journal = {Ars Journal},
	number = {10},
	pages = {947--954},
	title = {Gradient theory of optimal flight paths},
	volume = {30},
	year = {1960}}

@article{Ruder2016,
	author = {Ruder, Sebastian},
	date-added = {2021-09-16 12:35:04 -0300},
	date-modified = {2021-09-16 12:35:04 -0300},
	journal = {arXiv preprint arXiv:1609.04747},
	title = {An overview of gradient descent optimization algorithms},
	year = {2016}}

@article{Thuerey2021,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {https://ui.adsabs.harvard.edu/abs/2021arXiv210905237T},
	archiveprefix = {arXiv},
	author = {{Thuerey}, Nils and {Holl}, Philipp and {Mueller}, Maximilian and {Schnell}, Patrick and {Trost}, Felix and {Um}, Kiwon},
	date-added = {2021-09-15 14:13:06 -0300},
	date-modified = {2021-09-15 14:13:09 -0300},
	eid = {arXiv:2109.05237},
	eprint = {2109.05237},
	journal = {arXiv e-prints},
	keywords = {Computer Science - Machine Learning, Physics - Computational Physics},
	month = sep,
	pages = {arXiv:2109.05237},
	primaryclass = {cs.LG},
	title = {{Physics-based Deep Learning}},
	year = 2021}

@article{Breen2020,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {https://ui.adsabs.harvard.edu/abs/2020MNRAS.494.2465B},
	archiveprefix = {arXiv},
	author = {{Breen}, Philip G. and {Foley}, Christopher N. and {Boekholt}, Tjarda and {Portegies Zwart}, Simon},
	date-added = {2021-04-16 15:14:54 -0300},
	date-modified = {2021-04-16 15:14:55 -0300},
	doi = {10.1093/mnras/staa713},
	eprint = {1910.07291},
	journal = {\mnras},
	keywords = {methods: numerical, methods: statistical, Astrophysics - Astrophysics of Galaxies, Astrophysics - Solar and Stellar Astrophysics, Computer Science - Machine Learning, Physics - Computational Physics},
	month = may,
	number = {2},
	pages = {2465-2470},
	primaryclass = {astro-ph.GA},
	title = {{Newton versus the machine: solving the chaotic three-body problem using deep neural networks}},
	volume = {494},
	year = 2020,
	bdsk-url-1 = {https://doi.org/10.1093/mnras/staa713}}

@book{Goodfellow2016,
	author = {Ian Goodfellow and Yoshua Bengio and Aaron Courville},
	date-added = {2020-05-26 15:50:28 -0300},
	date-modified = {2020-05-26 15:50:30 -0300},
	note = {\url{http://www.deeplearningbook.org}},
	publisher = {MIT Press},
	title = {Deep Learning},
	year = {2016}}

@misc{Mohan2020,
	archiveprefix = {arXiv},
	author = {Arvind T. Mohan and Nicholas Lubbers and Daniel Livescu and Michael Chertkov},
	date-added = {2020-04-28 11:48:14 -0300},
	date-modified = {2020-04-28 11:48:15 -0300},
	eprint = {2002.00021},
	primaryclass = {physics.comp-ph},
	title = {Embedding Hard Physical Constraints in Neural Network Coarse-Graining of 3D Turbulence},
	year = {2020}}

@article{King2018,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv181007785K},
	archiveprefix = {arXiv},
	author = {{King}, Ryan and {Hennigh}, Oliver and {Mohan}, Arvind and {Chertkov}, Michael},
	date-added = {2020-04-24 15:28:43 -0300},
	date-modified = {2020-04-24 15:28:44 -0300},
	eid = {arXiv:1810.07785},
	eprint = {1810.07785},
	journal = {arXiv e-prints},
	keywords = {Physics - Fluid Dynamics, Computer Science - Machine Learning, Nonlinear Sciences - Chaotic Dynamics, Statistics - Machine Learning},
	month = oct,
	pages = {arXiv:1810.07785},
	primaryclass = {physics.flu-dyn},
	title = {{From Deep to Physics-Informed Learning of Turbulence: Diagnostics}},
	year = 2018}

@inproceedings{Battaglia2016,
	address = {Red Hook, NY, USA},
	author = {Battaglia, Peter and Pascanu, Razvan and Lai, Matthew and Rezende, Danilo Jimenez and kavukcuoglu, Koray},
	booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
	date-added = {2020-04-24 15:26:41 -0300},
	date-modified = {2020-04-24 15:26:42 -0300},
	isbn = {9781510838819},
	location = {Barcelona, Spain},
	numpages = {9},
	pages = {4509--4517},
	publisher = {Curran Associates Inc.},
	series = {NIPS'16},
	title = {Interaction Networks for Learning about Objects, Relations and Physics},
	year = {2016}}

@article{Chattopadhyay2020,
	abstract = {Abstract Numerical weather prediction models require ever-growing computing time and resources but, still, have sometimes difficulties with predicting weather extremes. We introduce a data-driven framework that is based on analog forecasting (prediction using past similar patterns) and employs a novel deep learning pattern-recognition technique (capsule neural networks, CapsNets) and an impact-based autolabeling strategy. Using data from a large-ensemble fully coupled Earth system model, CapsNets are trained on midtropospheric large-scale circulation patterns (Z500) labeled 0--4 depending on the existence and geographical region of surface temperature extremes over North America several days ahead. The trained networks predict the occurrence/region of cold or heat waves, only using Z500, with accuracies (recalls) of 69--45\% (77--48\%) or 62--41\% (73--47\%) 1--5 days ahead. Using both surface temperature and Z500, accuracies (recalls) with CapsNets increase to 80\% (88\%). In both cases, CapsNets outperform simpler techniques such as convolutional neural networks and logistic regression, and their accuracy is least affected as the size of the training set is reduced. The results show the promises of multivariate data-driven frameworks for accurate and fast extreme weather predictions, which can potentially augment numerical weather prediction efforts in providing early warnings.},
	author = {Chattopadhyay, Ashesh and Nabizadeh, Ebrahim and Hassanzadeh, Pedram},
	date-added = {2020-04-24 07:01:19 -0300},
	date-modified = {2020-04-24 07:01:21 -0300},
	doi = {10.1029/2019MS001958},
	eprint = {https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2019MS001958},
	journal = {Journal of Advances in Modeling Earth Systems},
	keywords = {extreme weather events, deep learning, analog forecasting, weather prediction, data-driven modeling},
	note = {e2019MS001958 10.1029/2019MS001958},
	number = {2},
	pages = {e2019MS001958},
	title = {Analog Forecasting of Extreme-Causing Weather Patterns Using Deep Learning},
	url = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2019MS001958},
	volume = {12},
	year = {2020},
	bdsk-url-1 = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2019MS001958},
	bdsk-url-2 = {https://doi.org/10.1029/2019MS001958}}

@article{Krizhevsky2017,
	address = {New York, NY, USA},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	date-added = {2020-04-23 17:46:54 -0300},
	date-modified = {2020-04-23 17:46:55 -0300},
	doi = {10.1145/3065386},
	issn = {0001-0782},
	issue_date = {May 2017},
	journal = {Commun. ACM},
	month = may,
	number = {6},
	numpages = {7},
	pages = {84--90},
	publisher = {Association for Computing Machinery},
	title = {ImageNet Classification with Deep Convolutional Neural Networks},
	url = {https://doi.org/10.1145/3065386},
	volume = {60},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1145/3065386}}

@article{Zhou2020,
	abstract = {Deep learning has been widely applied and brought breakthroughs in speech recognition, computer vision, and many other domains. Deep neural network architectures and computational issues have been well studied in machine learning. But there lacks a theoretical foundation for understanding the approximation or generalization ability of deep learning methods generated by the network architectures such as deep convolutional neural networks. Here we show that a deep convolutional neural network (CNN) is universal, meaning that it can be used to approximate any continuous function to an arbitrary accuracy when the depth of the neural network is large enough. This answers an open question in learning theory. Our quantitative estimate, given tightly in terms of the number of free parameters to be computed, verifies the efficiency of deep CNNs in dealing with large dimensional data. Our study also demonstrates the role of convolutions in deep CNNs.},
	author = {Ding-Xuan Zhou},
	date-added = {2020-04-23 17:39:37 -0300},
	date-modified = {2020-04-23 17:39:38 -0300},
	doi = {https://doi.org/10.1016/j.acha.2019.06.004},
	issn = {1063-5203},
	journal = {Applied and Computational Harmonic Analysis},
	keywords = {Deep learning, Convolutional neural network, Universality, Approximation theory},
	number = {2},
	pages = {787 - 794},
	title = {Universality of deep convolutional neural networks},
	url = {http://www.sciencedirect.com/science/article/pii/S1063520318302045},
	volume = {48},
	year = {2020},
	bdsk-url-1 = {http://www.sciencedirect.com/science/article/pii/S1063520318302045},
	bdsk-url-2 = {https://doi.org/10.1016/j.acha.2019.06.004}}

@article{Hornik1991,
	abstract = {We show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to Lp(μ) performance criteria, for arbitrary finite input environment measures μ, provided only that sufficiently many hidden units are available. If the activation function is continuous, bounded and nonconstant, then continuous mappings can be learned uniformly over compact input sets. We also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives.},
	author = {Kurt Hornik},
	date-added = {2020-04-23 17:39:08 -0300},
	date-modified = {2020-04-23 17:39:09 -0300},
	doi = {https://doi.org/10.1016/0893-6080(91)90009-T},
	issn = {0893-6080},
	journal = {Neural Networks},
	keywords = {Multilayer feedforward networks, Activation function, Universal approximation capabilities, Input environment measure, () approximation, Uniform approximation, Sobolev spaces, Smooth approximation},
	number = {2},
	pages = {251 - 257},
	title = {Approximation capabilities of multilayer feedforward networks},
	url = {http://www.sciencedirect.com/science/article/pii/089360809190009T},
	volume = {4},
	year = {1991},
	bdsk-url-1 = {http://www.sciencedirect.com/science/article/pii/089360809190009T},
	bdsk-url-2 = {https://doi.org/10.1016/0893-6080(91)90009-T}}

@article{Cybenko1989,
	abstract = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
	author = {Cybenko, G.},
	da = {1989/12/01},
	date-added = {2020-04-23 17:33:37 -0300},
	date-modified = {2020-04-23 17:33:38 -0300},
	doi = {10.1007/BF02551274},
	id = {Cybenko1989},
	isbn = {1435-568X},
	journal = {Mathematics of Control, Signals and Systems},
	number = {4},
	pages = {303--314},
	title = {Approximation by superpositions of a sigmoidal function},
	ty = {JOUR},
	url = {https://doi.org/10.1007/BF02551274},
	volume = {2},
	year = {1989},
	bdsk-url-1 = {https://doi.org/10.1007/BF02551274}}

@article{LeCun2015,
	abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	da = {2015/05/01},
	date-added = {2020-04-23 17:22:35 -0300},
	date-modified = {2020-04-23 17:22:36 -0300},
	doi = {10.1038/nature14539},
	id = {LeCun2015},
	isbn = {1476-4687},
	journal = {Nature},
	number = {7553},
	pages = {436--444},
	title = {Deep learning},
	ty = {JOUR},
	url = {https://doi.org/10.1038/nature14539},
	volume = {521},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1038/nature14539}}

@misc{Hausen2019,
	archiveprefix = {arXiv},
	author = {Ryan Hausen and Brant Robertson},
	date-added = {2020-04-23 16:38:45 -0300},
	date-modified = {2020-04-23 16:38:47 -0300},
	eprint = {1906.11248},
	primaryclass = {astro-ph.GA},
	title = {Morpheus: A Deep Learning Framework For Pixel-Level Analysis of Astronomical Image Data},
	year = {2019}}

@misc{Zhang2019,
	archiveprefix = {arXiv},
	author = {Xinyue Zhang and Yanfang Wang and Wei Zhang and Yueqiu Sun and Siyu He and Gabriella Contardo and Francisco Villaescusa-Navarro and Shirley Ho},
	date-added = {2020-04-23 15:59:28 -0300},
	date-modified = {2020-04-23 15:59:30 -0300},
	eprint = {1902.05965},
	primaryclass = {astro-ph.CO},
	title = {From Dark Matter to Galaxies with Convolutional Networks},
	year = {2019}}

@article{He2019,
	abstract = {To understand the evolution of the Universe requires a concerted effort of accurate observation of the sky and fast prediction of structures in the Universe. N-body simulation is an effective approach to predicting structure formation of the Universe, though computationally expensive. Here, we build a deep neural network to predict structure formation of the Universe. It outperforms the traditional fast-analytical approximation and accurately extrapolates far beyond its training data. Our study proves that deep learning is an accurate alternative to the traditional way of generating approximate cosmological simulations. Our study shows that one can use deep learning to generate complex 3D simulations in cosmology. This suggests that deep learning can provide a powerful alternative to traditional numerical simulations in cosmology.Matter evolved under the influence of gravity from minuscule density fluctuations. Nonperturbative structure formed hierarchically over all scales and developed non-Gaussian features in the Universe, known as the cosmic web. To fully understand the structure formation of the Universe is one of the holy grails of modern astrophysics. Astrophysicists survey large volumes of the Universe and use a large ensemble of computer simulations to compare with the observed data to extract the full information of our own Universe. However, to evolve billions of particles over billions of years, even with the simplest physics, is a daunting task. We build a deep neural network, the Deep Density Displacement Model (D3M), which learns from a set of prerun numerical simulations, to predict the nonlinear large-scale structure of the Universe with the Zel{\textquoteright}dovich Approximation (ZA), an analytical approximation based on perturbation theory, as the input. Our extensive analysis demonstrates that D3M outperforms the second-order perturbation theory (2LPT), the commonly used fast-approximate simulation method, in predicting cosmic structure in the nonlinear regime. We also show that D3M is able to accurately extrapolate far beyond its training data and predict structure formation for significantly different cosmological parameters. Our study proves that deep learning is a practical and accurate alternative to approximate 3D simulations of the gravitational structure formation of the Universe.},
	author = {He, Siyu and Li, Yin and Feng, Yu and Ho, Shirley and Ravanbakhsh, Siamak and Chen, Wei and P{\'o}czos, Barnab{\'a}s},
	date-added = {2020-04-23 15:57:39 -0300},
	date-modified = {2020-04-23 15:57:40 -0300},
	doi = {10.1073/pnas.1821458116},
	eprint = {https://www.pnas.org/content/116/28/13825.full.pdf},
	issn = {0027-8424},
	journal = {Proceedings of the National Academy of Sciences},
	number = {28},
	pages = {13825--13832},
	publisher = {National Academy of Sciences},
	title = {Learning to predict the cosmological structure formation},
	url = {https://www.pnas.org/content/116/28/13825},
	volume = {116},
	year = {2019},
	bdsk-url-1 = {https://www.pnas.org/content/116/28/13825},
	bdsk-url-2 = {https://doi.org/10.1073/pnas.1821458116}}

@misc{Mohan2019,
	archiveprefix = {arXiv},
	author = {Arvind Mohan and Don Daniel and Michael Chertkov and Daniel Livescu},
	date-added = {2020-04-23 15:48:22 -0300},
	date-modified = {2020-04-23 15:48:23 -0300},
	eprint = {1903.00033},
	primaryclass = {physics.flu-dyn},
	title = {Compressed Convolutional LSTM: An Efficient Deep Learning framework to Model High Fidelity 3D Turbulence},
	year = {2019}}

@misc{Tompson2016,
	archiveprefix = {arXiv},
	author = {Jonathan Tompson and Kristofer Schlachter and Pablo Sprechmann and Ken Perlin},
	date-added = {2020-04-23 15:36:35 -0300},
	date-modified = {2020-04-23 15:36:36 -0300},
	eprint = {1607.03597},
	primaryclass = {cs.CV},
	title = {Accelerating Eulerian Fluid Simulation With Convolutional Networks},
	year = {2016}}

@article{Jaeger2004,
	abstract = {We present a method for learning nonlinear systems, echo state networks (ESNs). ESNs employ artificial recurrent neural networks in a way that has recently been proposed independently as a learning mechanism in biological brains. The learning method is computationally efficient and easy to use. On a benchmark task of predicting a chaotic time series, accuracy is improved by a factor of 2400 over previous techniques. The potential for engineering applications is illustrated by equalizing a communication channel, where the signal error rate is improved by two orders of magnitude.},
	author = {Jaeger, Herbert and Haas, Harald},
	date-added = {2020-04-23 15:34:32 -0300},
	date-modified = {2020-04-23 15:34:33 -0300},
	doi = {10.1126/science.1091277},
	eprint = {https://science.sciencemag.org/content/304/5667/78.full.pdf},
	issn = {0036-8075},
	journal = {Science},
	number = {5667},
	pages = {78--80},
	publisher = {American Association for the Advancement of Science},
	title = {Harnessing Nonlinearity: Predicting Chaotic Systems and Saving Energy in Wireless Communication},
	url = {https://science.sciencemag.org/content/304/5667/78},
	volume = {304},
	year = {2004},
	bdsk-url-1 = {https://science.sciencemag.org/content/304/5667/78},
	bdsk-url-2 = {https://doi.org/10.1126/science.1091277}}

@article{Breen2019,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv191007291B},
	archiveprefix = {arXiv},
	author = {{Breen}, Philip G. and {Foley}, Christopher N. and {Boekholt}, Tjarda and {Portegies Zwart}, Simon},
	date-added = {2020-04-23 15:31:18 -0300},
	date-modified = {2020-04-23 15:31:19 -0300},
	eid = {arXiv:1910.07291},
	eprint = {1910.07291},
	journal = {arXiv e-prints},
	keywords = {Astrophysics - Astrophysics of Galaxies, Astrophysics - Solar and Stellar Astrophysics, Computer Science - Machine Learning, Physics - Computational Physics},
	month = oct,
	pages = {arXiv:1910.07291},
	primaryclass = {astro-ph.GA},
	title = {{Newton vs the machine: solving the chaotic three-body problem using deep neural networks}},
	year = 2019}

@misc{Agrawal2019,
	archiveprefix = {arXiv},
	author = {Shreya Agrawal and Luke Barrington and Carla Bromberg and John Burge and Cenk Gazen and Jason Hickey},
	date-added = {2020-04-23 15:25:01 -0300},
	date-modified = {2020-04-23 15:25:03 -0300},
	eprint = {1912.12132},
	primaryclass = {cs.CV},
	title = {Machine Learning for Precipitation Nowcasting from Radar Images},
	year = {2019}}

@article{Brunton2020,
	abstract = { The field of fluid mechanics is rapidly advancing, driven by unprecedented volumes of data from experiments, field measurements, and large-scale simulations at multiple spatiotemporal scales. Machine learning (ML) offers a wealth of techniques to extract information from data that can be translated into knowledge about the underlying fluid mechanics. Moreover, ML algorithms can augment domain knowledge and automate tasks related to flow control and optimization. This article presents an overview of past history, current developments, and emerging opportunities of ML for fluid mechanics. We outline fundamental ML methodologies and discuss their uses for understanding, modeling, optimizing, and controlling fluid flows. The strengths and limitations of these methods are addressed from the perspective of scientific inquiry that considers data as an inherent part of modeling, experiments, and simulations. ML provides a powerful information-processing framework that can augment, and possibly even transform, current lines of fluid mechanics research and industrial applications. },
	author = {Brunton, Steven L. and Noack, Bernd R. and Koumoutsakos, Petros},
	date-added = {2020-04-22 19:29:56 -0300},
	date-modified = {2020-04-22 19:29:57 -0300},
	doi = {10.1146/annurev-fluid-010719-060214},
	eprint = {https://doi.org/10.1146/annurev-fluid-010719-060214},
	journal = {Annual Review of Fluid Mechanics},
	number = {1},
	pages = {477-508},
	title = {Machine Learning for Fluid Mechanics},
	url = {https://doi.org/10.1146/annurev-fluid-010719-060214},
	volume = {52},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1146/annurev-fluid-010719-060214}}

@article{Pathak2018,
	author = {Pathak, Jaideep and Hunt, Brian and Girvan, Michelle and Lu, Zhixin and Ott, Edward},
	date-added = {2019-12-26 15:54:03 -0300},
	date-modified = {2019-12-26 15:54:03 -0300},
	doi = {10.1103/PhysRevLett.120.024102},
	issue = {2},
	journal = {Phys. Rev. Lett.},
	month = {Jan},
	numpages = {5},
	pages = {024102},
	publisher = {American Physical Society},
	title = {Model-Free Prediction of Large Spatiotemporally Chaotic Systems from Data: A Reservoir Computing Approach},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.120.024102},
	volume = {120},
	year = {2018},
	bdsk-url-1 = {https://link.aps.org/doi/10.1103/PhysRevLett.120.024102},
	bdsk-url-2 = {https://doi.org/10.1103/PhysRevLett.120.024102}}

@article{Pathak2017,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2017Chaos..27l1102P},
	archiveprefix = {arXiv},
	author = {{Pathak}, J. and {Lu}, Z. and {Hunt}, B.~R. and {Girvan}, M. and {Ott}, E.},
	date-added = {2019-12-26 15:54:03 -0300},
	date-modified = {2019-12-26 15:54:03 -0300},
	doi = {10.1063/1.5010300},
	eid = {121102},
	eprint = {1710.07313},
	journal = {Chaos},
	month = dec,
	number = {12},
	pages = {121102},
	primaryclass = {nlin.CD},
	title = {{Using machine learning to replicate chaotic attractors and calculate Lyapunov exponents from data}},
	volume = {27},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1063/1.5010300}}

@misc{Iten2018,
	archiveprefix = {arXiv},
	author = {Raban Iten and Tony Metger and Henrik Wilming and Lidia del Rio and Renato Renner},
	date-added = {2019-12-25 17:18:52 -0300},
	date-modified = {2019-12-25 17:20:22 -0300},
	eprint = {1807.10300},
	primaryclass = {quant-ph},
	title = {Discovering physical concepts with neural networks},
	year = {2018}}

@article{Burger2018,
	abstract = {The use of quantum field theory to understand astrophysical phenomena is not new. However, for the most part, the methods used are those that have been developed decades ago. The intervening years have seen some remarkable developments in computational quantum field theoretic tools. In particle physics, this technology has facilitated calculations that, even ten years ago would have seemed laughably difficult. It is remarkable, then, that most of these new techniques have remained firmly within the domain of high energy physics. We would like to change this. As alluded to in the title, this paper is aimed at showcasing the use of modern on-shell methods in the context of astrophysics and cosmology. In this article, we use the old problem of the bending of light by a compact object as an anchor to pedagogically develop these new computational tools. Once developed, we then illustrate their power and utility with an application to the scattering of gravitational waves.},
	author = {Burger, Daniel J. and Carballo-Rubio, Ra{\'u}l and Moynihan, Nathan and Murugan, Jeff and Weltman, Amanda},
	da = {2018/11/17},
	date-added = {2019-12-20 10:27:53 -0300},
	date-modified = {2019-12-20 10:31:01 -0300},
	doi = {10.1007/s10714-018-2475-0},
	id = {Burger2018},
	isbn = {1572-9532},
	journal = {General Relativity and Gravitation},
	number = {12},
	pages = {156},
	title = {Amplitudes for astrophysicists: known knowns},
	ty = {JOUR},
	url = {https://doi.org/10.1007/s10714-018-2475-0},
	volume = {50},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1007/s10714-018-2475-0}}

@article{Silver2016,
	abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses `value networks' to evaluate board positions and `policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2016Natur.529..484S},
	author = {{Silver}, D. and {Huang}, A. and {Maddison}, C.~J. and {Guez}, A. and {Sifre}, L. and {van den Driessche}, G. and {Schrittwieser}, J. and {Antonoglou}, I. and {Panneershelvam}, V. and {Lanctot}, M. and {Dieleman}, S. and {Grewe}, D. and {Nham}, J. and {Kalchbrenner}, N. and {Sutskever}, I. and {Lillicrap}, T. and {Leach}, M. and {Kavukcuoglu}, K. and {Graepel}, T. and {Hassabis}, D.},
	doi = {10.1038/nature16961},
	journal = {\nat},
	month = jan,
	pages = {484-489},
	title = {{Mastering the game of Go with deep neural networks and tree search}},
	volume = {529},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1038/nature16961}}

@article{Radford2019,
	author = {Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
	title = {Language Models are Unsupervised Multitask Learners},
	year = {2019}}

@incollection{Mnih2013,
	author = {Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin Riedmiller},
	booktitle = {NIPS Deep Learning Workshop},
	title = {Playing Atari With Deep Reinforcement Learning},
	year = {2013}}

@article{Mnih2015,
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
	day = {26},
	journal = {Nature},
	month = {02},
	number = {7540},
	pages = {529--533},
	title = {Human-level control through deep reinforcement learning},
	url = {http://dx.doi.org/10.1038/nature14236},
	volume = {518},
	year = {2015},
	bdsk-url-1 = {http://dx.doi.org/10.1038/nature14236}}

@article{Barchi2020,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {https://ui.adsabs.harvard.edu/abs/2020A&C....3000334B},
	author = {{Barchi}, P.~H. and {de Carvalho}, R.~R. and {Rosa}, R.~R. and {Sautter}, R.~A. and {Soares-Santos}, M. and {Marques}, B.~A.~D. and {Clua}, E. and {Gon{\c{c}}alves}, T.~S. and {de S{\'a}-Freitas}, C. and {Moura}, T.~C.},
	doi = {10.1016/j.ascom.2019.100334},
	eid = {100334},
	journal = {Astronomy and Computing},
	keywords = {Galaxies: photometry, Methods: data analysis, Machine learning, Techniques: image processing, Galaxies: General, Catalogs},
	month = {Jan},
	pages = {100334},
	title = {{Machine and Deep Learning applied to galaxy morphology - A comparative study}},
	volume = {30},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1016/j.ascom.2019.100334}}

@article{George2018,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {https://ui.adsabs.harvard.edu/abs/2018PhRvD..97d4039G},
	archiveprefix = {arXiv},
	author = {{George}, Daniel and {Huerta}, E.~A.},
	doi = {10.1103/PhysRevD.97.044039},
	eid = {044039},
	eprint = {1701.00008},
	journal = {\prd},
	keywords = {Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Astrophysics of Galaxies, Astrophysics - High Energy Astrophysical Phenomena, Computer Science - Machine Learning, General Relativity and Quantum Cosmology},
	month = {Feb},
	number = {4},
	pages = {044039},
	primaryclass = {astro-ph.IM},
	title = {{Deep neural networks to enable real-time multimessenger astrophysics}},
	volume = {97},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1103/PhysRevD.97.044039}}

@article{Schawinski2017,
	__markedentry = {[nemmen:6]},
	abstract = {{Observations of astrophysical objects such as galaxies are limited by various sources of random and systematic noise from the sky background, the optical system of the telescope and the detector used to record the data. Conventional deconvolution techniques are limited in their ability to recover features in imaging data by the Shannon--Nyquist sampling theorem. Here, we train a generative adversarial network (GAN) on a sample of 4550 images of nearby galaxies at 0.01 \\&lt; z \\&lt; 0.02 from the Sloan Digital Sky Survey and conduct 10× cross-validation to evaluate the results. We present a method using a GAN trained on galaxy images that can recover features from artificially degraded images with worse seeing and higher noise than the original with a performance that far exceeds simple deconvolution. The ability to better recover detailed features such as galaxy morphology from low signal to noise and low angular resolution imaging data significantly increases our ability to study existing data sets of astrophysical objects as well as future observations with observatories such as the Large Synoptic Sky Telescope (LSST) and the Hubble and James Webb space telescopes.}},
	author = {Schawinski, Kevin and Zhang, Ce and Zhang, Hantian and Fowler, Lucas and Santhanam, Gokula Krishnan},
	doi = {10.1093/mnrasl/slx008},
	eprint = {http://oup.prod.sis.lan/mnrasl/article-pdf/467/1/L110/10730451/slx008.pdf},
	issn = {1745-3925},
	journal = {\mnras},
	month = {01},
	number = {1},
	pages = {L110-L114},
	title = {{Generative adversarial networks recover features in astrophysical images of galaxies beyond the deconvolution limit}},
	url = {https://doi.org/10.1093/mnrasl/slx008},
	volume = {467},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1093/mnrasl/slx008}}
