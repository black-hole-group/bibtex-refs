% Encoding: UTF-8

@Article{Silver2016,
  author   = {{Silver}, D. and {Huang}, A. and {Maddison}, C.~J. and {Guez}, A. and {Sifre}, L. and {van den Driessche}, G. and {Schrittwieser}, J. and {Antonoglou}, I. and {Panneershelvam}, V. and {Lanctot}, M. and {Dieleman}, S. and {Grewe}, D. and {Nham}, J. and {Kalchbrenner}, N. and {Sutskever}, I. and {Lillicrap}, T. and {Leach}, M. and {Kavukcuoglu}, K. and {Graepel}, T. and {Hassabis}, D.},
  title    = {{Mastering the game of Go with deep neural networks and tree search}},
  journal  = {\nat},
  year     = {2016},
  volume   = {529},
  pages    = {484-489},
  month    = jan,
  abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses `value networks' to evaluate board positions and `policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
  adsnote  = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl   = {http://adsabs.harvard.edu/abs/2016Natur.529..484S},
  doi      = {10.1038/nature16961},
}

@Article{Radford2019,
  author = {Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  title  = {Language Models are Unsupervised Multitask Learners},
  year   = {2019},
}

@InCollection{Mnih2013,
  author    = {Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin Riedmiller},
  title     = {Playing Atari With Deep Reinforcement Learning},
  booktitle = {NIPS Deep Learning Workshop},
  year      = {2013},
}

@Article{Mnih2015,
  author  = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  title   = {Human-level control through deep reinforcement learning},
  journal = {Nature},
  year    = {2015},
  volume  = {518},
  number  = {7540},
  pages   = {529--533},
  month   = {02},
  day     = {26},
  url     = {http://dx.doi.org/10.1038/nature14236},
}

@Comment{jabref-meta: databaseType:bibtex;}
