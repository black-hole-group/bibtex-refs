%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Rodrigo Nemmen da Silva at 2020-05-26 15:50:33 -0300 


%% Saved with string encoding Unicode (UTF-8) 


@comment{jabref-meta: databaseType:bibtex;}



@book{Goodfellow2016,
	Author = {Ian Goodfellow and Yoshua Bengio and Aaron Courville},
	Date-Added = {2020-05-26 15:50:28 -0300},
	Date-Modified = {2020-05-26 15:50:30 -0300},
	Note = {\url{http://www.deeplearningbook.org}},
	Publisher = {MIT Press},
	Title = {Deep Learning},
	Year = {2016}}

@misc{Mohan2020,
	Archiveprefix = {arXiv},
	Author = {Arvind T. Mohan and Nicholas Lubbers and Daniel Livescu and Michael Chertkov},
	Date-Added = {2020-04-28 11:48:14 -0300},
	Date-Modified = {2020-04-28 11:48:15 -0300},
	Eprint = {2002.00021},
	Primaryclass = {physics.comp-ph},
	Title = {Embedding Hard Physical Constraints in Neural Network Coarse-Graining of 3D Turbulence},
	Year = {2020}}

@article{King2018,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv181007785K},
	Archiveprefix = {arXiv},
	Author = {{King}, Ryan and {Hennigh}, Oliver and {Mohan}, Arvind and {Chertkov}, Michael},
	Date-Added = {2020-04-24 15:28:43 -0300},
	Date-Modified = {2020-04-24 15:28:44 -0300},
	Eid = {arXiv:1810.07785},
	Eprint = {1810.07785},
	Journal = {arXiv e-prints},
	Keywords = {Physics - Fluid Dynamics, Computer Science - Machine Learning, Nonlinear Sciences - Chaotic Dynamics, Statistics - Machine Learning},
	Month = oct,
	Pages = {arXiv:1810.07785},
	Primaryclass = {physics.flu-dyn},
	Title = {{From Deep to Physics-Informed Learning of Turbulence: Diagnostics}},
	Year = 2018}

@inproceedings{Battaglia2016,
	Address = {Red Hook, NY, USA},
	Author = {Battaglia, Peter and Pascanu, Razvan and Lai, Matthew and Rezende, Danilo Jimenez and kavukcuoglu, Koray},
	Booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
	Date-Added = {2020-04-24 15:26:41 -0300},
	Date-Modified = {2020-04-24 15:26:42 -0300},
	Isbn = {9781510838819},
	Location = {Barcelona, Spain},
	Numpages = {9},
	Pages = {4509--4517},
	Publisher = {Curran Associates Inc.},
	Series = {NIPS'16},
	Title = {Interaction Networks for Learning about Objects, Relations and Physics},
	Year = {2016}}

@article{Chattopadhyay2020,
	Abstract = {Abstract Numerical weather prediction models require ever-growing computing time and resources but, still, have sometimes difficulties with predicting weather extremes. We introduce a data-driven framework that is based on analog forecasting (prediction using past similar patterns) and employs a novel deep learning pattern-recognition technique (capsule neural networks, CapsNets) and an impact-based autolabeling strategy. Using data from a large-ensemble fully coupled Earth system model, CapsNets are trained on midtropospheric large-scale circulation patterns (Z500) labeled 0--4 depending on the existence and geographical region of surface temperature extremes over North America several days ahead. The trained networks predict the occurrence/region of cold or heat waves, only using Z500, with accuracies (recalls) of 69--45\% (77--48\%) or 62--41\% (73--47\%) 1--5 days ahead. Using both surface temperature and Z500, accuracies (recalls) with CapsNets increase to 80\% (88\%). In both cases, CapsNets outperform simpler techniques such as convolutional neural networks and logistic regression, and their accuracy is least affected as the size of the training set is reduced. The results show the promises of multivariate data-driven frameworks for accurate and fast extreme weather predictions, which can potentially augment numerical weather prediction efforts in providing early warnings.},
	Author = {Chattopadhyay, Ashesh and Nabizadeh, Ebrahim and Hassanzadeh, Pedram},
	Date-Added = {2020-04-24 07:01:19 -0300},
	Date-Modified = {2020-04-24 07:01:21 -0300},
	Doi = {10.1029/2019MS001958},
	Eprint = {https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2019MS001958},
	Journal = {Journal of Advances in Modeling Earth Systems},
	Keywords = {extreme weather events, deep learning, analog forecasting, weather prediction, data-driven modeling},
	Note = {e2019MS001958 10.1029/2019MS001958},
	Number = {2},
	Pages = {e2019MS001958},
	Title = {Analog Forecasting of Extreme-Causing Weather Patterns Using Deep Learning},
	Url = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2019MS001958},
	Volume = {12},
	Year = {2020},
	Bdsk-Url-1 = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2019MS001958},
	Bdsk-Url-2 = {https://doi.org/10.1029/2019MS001958}}

@article{Krizhevsky2017,
	Address = {New York, NY, USA},
	Author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	Date-Added = {2020-04-23 17:46:54 -0300},
	Date-Modified = {2020-04-23 17:46:55 -0300},
	Doi = {10.1145/3065386},
	Issn = {0001-0782},
	Issue_Date = {May 2017},
	Journal = {Commun. ACM},
	Month = may,
	Number = {6},
	Numpages = {7},
	Pages = {84--90},
	Publisher = {Association for Computing Machinery},
	Title = {ImageNet Classification with Deep Convolutional Neural Networks},
	Url = {https://doi.org/10.1145/3065386},
	Volume = {60},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1145/3065386}}

@article{Zhou2020,
	Abstract = {Deep learning has been widely applied and brought breakthroughs in speech recognition, computer vision, and many other domains. Deep neural network architectures and computational issues have been well studied in machine learning. But there lacks a theoretical foundation for understanding the approximation or generalization ability of deep learning methods generated by the network architectures such as deep convolutional neural networks. Here we show that a deep convolutional neural network (CNN) is universal, meaning that it can be used to approximate any continuous function to an arbitrary accuracy when the depth of the neural network is large enough. This answers an open question in learning theory. Our quantitative estimate, given tightly in terms of the number of free parameters to be computed, verifies the efficiency of deep CNNs in dealing with large dimensional data. Our study also demonstrates the role of convolutions in deep CNNs.},
	Author = {Ding-Xuan Zhou},
	Date-Added = {2020-04-23 17:39:37 -0300},
	Date-Modified = {2020-04-23 17:39:38 -0300},
	Doi = {https://doi.org/10.1016/j.acha.2019.06.004},
	Issn = {1063-5203},
	Journal = {Applied and Computational Harmonic Analysis},
	Keywords = {Deep learning, Convolutional neural network, Universality, Approximation theory},
	Number = {2},
	Pages = {787 - 794},
	Title = {Universality of deep convolutional neural networks},
	Url = {http://www.sciencedirect.com/science/article/pii/S1063520318302045},
	Volume = {48},
	Year = {2020},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S1063520318302045},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.acha.2019.06.004}}

@article{Hornik1991,
	Abstract = {We show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to Lp(μ) performance criteria, for arbitrary finite input environment measures μ, provided only that sufficiently many hidden units are available. If the activation function is continuous, bounded and nonconstant, then continuous mappings can be learned uniformly over compact input sets. We also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives.},
	Author = {Kurt Hornik},
	Date-Added = {2020-04-23 17:39:08 -0300},
	Date-Modified = {2020-04-23 17:39:09 -0300},
	Doi = {https://doi.org/10.1016/0893-6080(91)90009-T},
	Issn = {0893-6080},
	Journal = {Neural Networks},
	Keywords = {Multilayer feedforward networks, Activation function, Universal approximation capabilities, Input environment measure, () approximation, Uniform approximation, Sobolev spaces, Smooth approximation},
	Number = {2},
	Pages = {251 - 257},
	Title = {Approximation capabilities of multilayer feedforward networks},
	Url = {http://www.sciencedirect.com/science/article/pii/089360809190009T},
	Volume = {4},
	Year = {1991},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/089360809190009T},
	Bdsk-Url-2 = {https://doi.org/10.1016/0893-6080(91)90009-T}}

@article{Cybenko1989,
	Abstract = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
	Author = {Cybenko, G.},
	Da = {1989/12/01},
	Date-Added = {2020-04-23 17:33:37 -0300},
	Date-Modified = {2020-04-23 17:33:38 -0300},
	Doi = {10.1007/BF02551274},
	Id = {Cybenko1989},
	Isbn = {1435-568X},
	Journal = {Mathematics of Control, Signals and Systems},
	Number = {4},
	Pages = {303--314},
	Title = {Approximation by superpositions of a sigmoidal function},
	Ty = {JOUR},
	Url = {https://doi.org/10.1007/BF02551274},
	Volume = {2},
	Year = {1989},
	Bdsk-Url-1 = {https://doi.org/10.1007/BF02551274}}

@article{LeCun2015,
	Abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
	Author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	Da = {2015/05/01},
	Date-Added = {2020-04-23 17:22:35 -0300},
	Date-Modified = {2020-04-23 17:22:36 -0300},
	Doi = {10.1038/nature14539},
	Id = {LeCun2015},
	Isbn = {1476-4687},
	Journal = {Nature},
	Number = {7553},
	Pages = {436--444},
	Title = {Deep learning},
	Ty = {JOUR},
	Url = {https://doi.org/10.1038/nature14539},
	Volume = {521},
	Year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1038/nature14539}}

@misc{Hausen2019,
	Archiveprefix = {arXiv},
	Author = {Ryan Hausen and Brant Robertson},
	Date-Added = {2020-04-23 16:38:45 -0300},
	Date-Modified = {2020-04-23 16:38:47 -0300},
	Eprint = {1906.11248},
	Primaryclass = {astro-ph.GA},
	Title = {Morpheus: A Deep Learning Framework For Pixel-Level Analysis of Astronomical Image Data},
	Year = {2019}}

@misc{Zhang2019,
	Archiveprefix = {arXiv},
	Author = {Xinyue Zhang and Yanfang Wang and Wei Zhang and Yueqiu Sun and Siyu He and Gabriella Contardo and Francisco Villaescusa-Navarro and Shirley Ho},
	Date-Added = {2020-04-23 15:59:28 -0300},
	Date-Modified = {2020-04-23 15:59:30 -0300},
	Eprint = {1902.05965},
	Primaryclass = {astro-ph.CO},
	Title = {From Dark Matter to Galaxies with Convolutional Networks},
	Year = {2019}}

@article{He2019,
	Abstract = {To understand the evolution of the Universe requires a concerted effort of accurate observation of the sky and fast prediction of structures in the Universe. N-body simulation is an effective approach to predicting structure formation of the Universe, though computationally expensive. Here, we build a deep neural network to predict structure formation of the Universe. It outperforms the traditional fast-analytical approximation and accurately extrapolates far beyond its training data. Our study proves that deep learning is an accurate alternative to the traditional way of generating approximate cosmological simulations. Our study shows that one can use deep learning to generate complex 3D simulations in cosmology. This suggests that deep learning can provide a powerful alternative to traditional numerical simulations in cosmology.Matter evolved under the influence of gravity from minuscule density fluctuations. Nonperturbative structure formed hierarchically over all scales and developed non-Gaussian features in the Universe, known as the cosmic web. To fully understand the structure formation of the Universe is one of the holy grails of modern astrophysics. Astrophysicists survey large volumes of the Universe and use a large ensemble of computer simulations to compare with the observed data to extract the full information of our own Universe. However, to evolve billions of particles over billions of years, even with the simplest physics, is a daunting task. We build a deep neural network, the Deep Density Displacement Model (D3M), which learns from a set of prerun numerical simulations, to predict the nonlinear large-scale structure of the Universe with the Zel{\textquoteright}dovich Approximation (ZA), an analytical approximation based on perturbation theory, as the input. Our extensive analysis demonstrates that D3M outperforms the second-order perturbation theory (2LPT), the commonly used fast-approximate simulation method, in predicting cosmic structure in the nonlinear regime. We also show that D3M is able to accurately extrapolate far beyond its training data and predict structure formation for significantly different cosmological parameters. Our study proves that deep learning is a practical and accurate alternative to approximate 3D simulations of the gravitational structure formation of the Universe.},
	Author = {He, Siyu and Li, Yin and Feng, Yu and Ho, Shirley and Ravanbakhsh, Siamak and Chen, Wei and P{\'o}czos, Barnab{\'a}s},
	Date-Added = {2020-04-23 15:57:39 -0300},
	Date-Modified = {2020-04-23 15:57:40 -0300},
	Doi = {10.1073/pnas.1821458116},
	Eprint = {https://www.pnas.org/content/116/28/13825.full.pdf},
	Issn = {0027-8424},
	Journal = {Proceedings of the National Academy of Sciences},
	Number = {28},
	Pages = {13825--13832},
	Publisher = {National Academy of Sciences},
	Title = {Learning to predict the cosmological structure formation},
	Url = {https://www.pnas.org/content/116/28/13825},
	Volume = {116},
	Year = {2019},
	Bdsk-Url-1 = {https://www.pnas.org/content/116/28/13825},
	Bdsk-Url-2 = {https://doi.org/10.1073/pnas.1821458116}}

@misc{Mohan2019,
	Archiveprefix = {arXiv},
	Author = {Arvind Mohan and Don Daniel and Michael Chertkov and Daniel Livescu},
	Date-Added = {2020-04-23 15:48:22 -0300},
	Date-Modified = {2020-04-23 15:48:23 -0300},
	Eprint = {1903.00033},
	Primaryclass = {physics.flu-dyn},
	Title = {Compressed Convolutional LSTM: An Efficient Deep Learning framework to Model High Fidelity 3D Turbulence},
	Year = {2019}}

@misc{Tompson2016,
	Archiveprefix = {arXiv},
	Author = {Jonathan Tompson and Kristofer Schlachter and Pablo Sprechmann and Ken Perlin},
	Date-Added = {2020-04-23 15:36:35 -0300},
	Date-Modified = {2020-04-23 15:36:36 -0300},
	Eprint = {1607.03597},
	Primaryclass = {cs.CV},
	Title = {Accelerating Eulerian Fluid Simulation With Convolutional Networks},
	Year = {2016}}

@article{Jaeger2004,
	Abstract = {We present a method for learning nonlinear systems, echo state networks (ESNs). ESNs employ artificial recurrent neural networks in a way that has recently been proposed independently as a learning mechanism in biological brains. The learning method is computationally efficient and easy to use. On a benchmark task of predicting a chaotic time series, accuracy is improved by a factor of 2400 over previous techniques. The potential for engineering applications is illustrated by equalizing a communication channel, where the signal error rate is improved by two orders of magnitude.},
	Author = {Jaeger, Herbert and Haas, Harald},
	Date-Added = {2020-04-23 15:34:32 -0300},
	Date-Modified = {2020-04-23 15:34:33 -0300},
	Doi = {10.1126/science.1091277},
	Eprint = {https://science.sciencemag.org/content/304/5667/78.full.pdf},
	Issn = {0036-8075},
	Journal = {Science},
	Number = {5667},
	Pages = {78--80},
	Publisher = {American Association for the Advancement of Science},
	Title = {Harnessing Nonlinearity: Predicting Chaotic Systems and Saving Energy in Wireless Communication},
	Url = {https://science.sciencemag.org/content/304/5667/78},
	Volume = {304},
	Year = {2004},
	Bdsk-Url-1 = {https://science.sciencemag.org/content/304/5667/78},
	Bdsk-Url-2 = {https://doi.org/10.1126/science.1091277}}

@article{Breen2019,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv191007291B},
	Archiveprefix = {arXiv},
	Author = {{Breen}, Philip G. and {Foley}, Christopher N. and {Boekholt}, Tjarda and {Portegies Zwart}, Simon},
	Date-Added = {2020-04-23 15:31:18 -0300},
	Date-Modified = {2020-04-23 15:31:19 -0300},
	Eid = {arXiv:1910.07291},
	Eprint = {1910.07291},
	Journal = {arXiv e-prints},
	Keywords = {Astrophysics - Astrophysics of Galaxies, Astrophysics - Solar and Stellar Astrophysics, Computer Science - Machine Learning, Physics - Computational Physics},
	Month = oct,
	Pages = {arXiv:1910.07291},
	Primaryclass = {astro-ph.GA},
	Title = {{Newton vs the machine: solving the chaotic three-body problem using deep neural networks}},
	Year = 2019}

@misc{Agrawal2019,
	Archiveprefix = {arXiv},
	Author = {Shreya Agrawal and Luke Barrington and Carla Bromberg and John Burge and Cenk Gazen and Jason Hickey},
	Date-Added = {2020-04-23 15:25:01 -0300},
	Date-Modified = {2020-04-23 15:25:03 -0300},
	Eprint = {1912.12132},
	Primaryclass = {cs.CV},
	Title = {Machine Learning for Precipitation Nowcasting from Radar Images},
	Year = {2019}}

@article{Brunton2020,
	Abstract = { The field of fluid mechanics is rapidly advancing, driven by unprecedented volumes of data from experiments, field measurements, and large-scale simulations at multiple spatiotemporal scales. Machine learning (ML) offers a wealth of techniques to extract information from data that can be translated into knowledge about the underlying fluid mechanics. Moreover, ML algorithms can augment domain knowledge and automate tasks related to flow control and optimization. This article presents an overview of past history, current developments, and emerging opportunities of ML for fluid mechanics. We outline fundamental ML methodologies and discuss their uses for understanding, modeling, optimizing, and controlling fluid flows. The strengths and limitations of these methods are addressed from the perspective of scientific inquiry that considers data as an inherent part of modeling, experiments, and simulations. ML provides a powerful information-processing framework that can augment, and possibly even transform, current lines of fluid mechanics research and industrial applications. },
	Author = {Brunton, Steven L. and Noack, Bernd R. and Koumoutsakos, Petros},
	Date-Added = {2020-04-22 19:29:56 -0300},
	Date-Modified = {2020-04-22 19:29:57 -0300},
	Doi = {10.1146/annurev-fluid-010719-060214},
	Eprint = {https://doi.org/10.1146/annurev-fluid-010719-060214},
	Journal = {Annual Review of Fluid Mechanics},
	Number = {1},
	Pages = {477-508},
	Title = {Machine Learning for Fluid Mechanics},
	Url = {https://doi.org/10.1146/annurev-fluid-010719-060214},
	Volume = {52},
	Year = {2020},
	Bdsk-Url-1 = {https://doi.org/10.1146/annurev-fluid-010719-060214}}

@article{Pathak2018,
	Author = {Pathak, Jaideep and Hunt, Brian and Girvan, Michelle and Lu, Zhixin and Ott, Edward},
	Date-Added = {2019-12-26 15:54:03 -0300},
	Date-Modified = {2019-12-26 15:54:03 -0300},
	Doi = {10.1103/PhysRevLett.120.024102},
	Issue = {2},
	Journal = {Phys. Rev. Lett.},
	Month = {Jan},
	Numpages = {5},
	Pages = {024102},
	Publisher = {American Physical Society},
	Title = {Model-Free Prediction of Large Spatiotemporally Chaotic Systems from Data: A Reservoir Computing Approach},
	Url = {https://link.aps.org/doi/10.1103/PhysRevLett.120.024102},
	Volume = {120},
	Year = {2018},
	Bdsk-Url-1 = {https://link.aps.org/doi/10.1103/PhysRevLett.120.024102},
	Bdsk-Url-2 = {https://doi.org/10.1103/PhysRevLett.120.024102}}

@article{Pathak2017,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017Chaos..27l1102P},
	Archiveprefix = {arXiv},
	Author = {{Pathak}, J. and {Lu}, Z. and {Hunt}, B.~R. and {Girvan}, M. and {Ott}, E.},
	Date-Added = {2019-12-26 15:54:03 -0300},
	Date-Modified = {2019-12-26 15:54:03 -0300},
	Doi = {10.1063/1.5010300},
	Eid = {121102},
	Eprint = {1710.07313},
	Journal = {Chaos},
	Month = dec,
	Number = {12},
	Pages = {121102},
	Primaryclass = {nlin.CD},
	Title = {{Using machine learning to replicate chaotic attractors and calculate Lyapunov exponents from data}},
	Volume = {27},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1063/1.5010300}}

@misc{Iten2018,
	Archiveprefix = {arXiv},
	Author = {Raban Iten and Tony Metger and Henrik Wilming and Lidia del Rio and Renato Renner},
	Date-Added = {2019-12-25 17:18:52 -0300},
	Date-Modified = {2019-12-25 17:20:22 -0300},
	Eprint = {1807.10300},
	Primaryclass = {quant-ph},
	Title = {Discovering physical concepts with neural networks},
	Year = {2018}}

@article{Burger2018,
	Abstract = {The use of quantum field theory to understand astrophysical phenomena is not new. However, for the most part, the methods used are those that have been developed decades ago. The intervening years have seen some remarkable developments in computational quantum field theoretic tools. In particle physics, this technology has facilitated calculations that, even ten years ago would have seemed laughably difficult. It is remarkable, then, that most of these new techniques have remained firmly within the domain of high energy physics. We would like to change this. As alluded to in the title, this paper is aimed at showcasing the use of modern on-shell methods in the context of astrophysics and cosmology. In this article, we use the old problem of the bending of light by a compact object as an anchor to pedagogically develop these new computational tools. Once developed, we then illustrate their power and utility with an application to the scattering of gravitational waves.},
	Author = {Burger, Daniel J. and Carballo-Rubio, Ra{\'u}l and Moynihan, Nathan and Murugan, Jeff and Weltman, Amanda},
	Da = {2018/11/17},
	Date-Added = {2019-12-20 10:27:53 -0300},
	Date-Modified = {2019-12-20 10:31:01 -0300},
	Doi = {10.1007/s10714-018-2475-0},
	Id = {Burger2018},
	Isbn = {1572-9532},
	Journal = {General Relativity and Gravitation},
	Number = {12},
	Pages = {156},
	Title = {Amplitudes for astrophysicists: known knowns},
	Ty = {JOUR},
	Url = {https://doi.org/10.1007/s10714-018-2475-0},
	Volume = {50},
	Year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1007/s10714-018-2475-0}}

@article{Silver2016,
	Abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses `value networks' to evaluate board positions and `policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016Natur.529..484S},
	Author = {{Silver}, D. and {Huang}, A. and {Maddison}, C.~J. and {Guez}, A. and {Sifre}, L. and {van den Driessche}, G. and {Schrittwieser}, J. and {Antonoglou}, I. and {Panneershelvam}, V. and {Lanctot}, M. and {Dieleman}, S. and {Grewe}, D. and {Nham}, J. and {Kalchbrenner}, N. and {Sutskever}, I. and {Lillicrap}, T. and {Leach}, M. and {Kavukcuoglu}, K. and {Graepel}, T. and {Hassabis}, D.},
	Doi = {10.1038/nature16961},
	Journal = {\nat},
	Month = jan,
	Pages = {484-489},
	Title = {{Mastering the game of Go with deep neural networks and tree search}},
	Volume = {529},
	Year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1038/nature16961}}

@article{Radford2019,
	Author = {Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
	Title = {Language Models are Unsupervised Multitask Learners},
	Year = {2019}}

@incollection{Mnih2013,
	Author = {Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin Riedmiller},
	Booktitle = {NIPS Deep Learning Workshop},
	Title = {Playing Atari With Deep Reinforcement Learning},
	Year = {2013}}

@article{Mnih2015,
	Author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
	Day = {26},
	Journal = {Nature},
	Month = {02},
	Number = {7540},
	Pages = {529--533},
	Title = {Human-level control through deep reinforcement learning},
	Url = {http://dx.doi.org/10.1038/nature14236},
	Volume = {518},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1038/nature14236}}

@article{Barchi2020,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {https://ui.adsabs.harvard.edu/abs/2020A&C....3000334B},
	Author = {{Barchi}, P.~H. and {de Carvalho}, R.~R. and {Rosa}, R.~R. and {Sautter}, R.~A. and {Soares-Santos}, M. and {Marques}, B.~A.~D. and {Clua}, E. and {Gon{\c{c}}alves}, T.~S. and {de S{\'a}-Freitas}, C. and {Moura}, T.~C.},
	Doi = {10.1016/j.ascom.2019.100334},
	Eid = {100334},
	Journal = {Astronomy and Computing},
	Keywords = {Galaxies: photometry, Methods: data analysis, Machine learning, Techniques: image processing, Galaxies: General, Catalogs},
	Month = {Jan},
	Pages = {100334},
	Title = {{Machine and Deep Learning applied to galaxy morphology - A comparative study}},
	Volume = {30},
	Year = {2020},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.ascom.2019.100334}}

@article{George2018,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {https://ui.adsabs.harvard.edu/abs/2018PhRvD..97d4039G},
	Archiveprefix = {arXiv},
	Author = {{George}, Daniel and {Huerta}, E.~A.},
	Doi = {10.1103/PhysRevD.97.044039},
	Eid = {044039},
	Eprint = {1701.00008},
	Journal = {\prd},
	Keywords = {Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Astrophysics of Galaxies, Astrophysics - High Energy Astrophysical Phenomena, Computer Science - Machine Learning, General Relativity and Quantum Cosmology},
	Month = {Feb},
	Number = {4},
	Pages = {044039},
	Primaryclass = {astro-ph.IM},
	Title = {{Deep neural networks to enable real-time multimessenger astrophysics}},
	Volume = {97},
	Year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1103/PhysRevD.97.044039}}

@article{Schawinski2017,
	__Markedentry = {[nemmen:6]},
	Abstract = {{Observations of astrophysical objects such as galaxies are limited by various sources of random and systematic noise from the sky background, the optical system of the telescope and the detector used to record the data. Conventional deconvolution techniques are limited in their ability to recover features in imaging data by the Shannon--Nyquist sampling theorem. Here, we train a generative adversarial network (GAN) on a sample of 4550 images of nearby galaxies at 0.01 \\&lt; z \\&lt; 0.02 from the Sloan Digital Sky Survey and conduct 10× cross-validation to evaluate the results. We present a method using a GAN trained on galaxy images that can recover features from artificially degraded images with worse seeing and higher noise than the original with a performance that far exceeds simple deconvolution. The ability to better recover detailed features such as galaxy morphology from low signal to noise and low angular resolution imaging data significantly increases our ability to study existing data sets of astrophysical objects as well as future observations with observatories such as the Large Synoptic Sky Telescope (LSST) and the Hubble and James Webb space telescopes.}},
	Author = {Schawinski, Kevin and Zhang, Ce and Zhang, Hantian and Fowler, Lucas and Santhanam, Gokula Krishnan},
	Doi = {10.1093/mnrasl/slx008},
	Eprint = {http://oup.prod.sis.lan/mnrasl/article-pdf/467/1/L110/10730451/slx008.pdf},
	Issn = {1745-3925},
	Journal = {\mnras},
	Month = {01},
	Number = {1},
	Pages = {L110-L114},
	Title = {{Generative adversarial networks recover features in astrophysical images of galaxies beyond the deconvolution limit}},
	Url = {https://doi.org/10.1093/mnrasl/slx008},
	Volume = {467},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1093/mnrasl/slx008}}
